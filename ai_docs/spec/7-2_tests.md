A truly exhaustive infrastructure‑level test suite for your experiment must exercise every moving part of the scaffolding exactly the way the language‑model agent will, while also probing all the failure paths you rely on for measurement, security, and clean shutdown. What follows is a prose specification of such a suite; no implementation code is included.

Begin with a “cold‑start” lineage test. Spin up a pristine host, fetch the repository on an empty filesystem, ensure that the only prerequisite is a working Docker installation, and invoke the build script. Confirm that the image builds deterministically from nothing more than the checked‑in source and that the layer digest is identical on two consecutive builds. As part of the same scenario, verify that the requirements resolver never attempts to reach the network by running the build behind a firewall and asserting success.

Next, exercise container boot integrity. Launch the image with the exact flags used by run_trial.py—read‑only root filesystem, --network none, the harness volume read‑only, the workspace volume read‑write—and with a deliberately truncated set of environment variables. Assert that the container exits early with a descriptive message if ANTHROPIC_API_KEY is missing, and that it refuses to start if the workspace mount is omitted or mounted read‑only. Repeat the boot with the correct environment and mounts and confirm that the entrypoint creates notes.md, .agent_state.json, initialises a git repository, commits “Initial state”, and then blocks awaiting an observation.

Proceed to an action‑protocol validation suite. Craft synthetic agent replies that cover every legal single‑action shape, each optional message field variant, and a handful of malformed JSON examples—bad UTF‑8, extra unknown keys, two primary keys at once, missing required fields, truncated UTF string, over‑long message. Feed each reply directly to the harness loop via a monkey‑patched Claude client that returns the canned text. Assert that valid actions reach the execution branch and malformed ones are rejected, logged with error_type = invalid_action, and that the harness continues rather than crashing.

Couple that with a patch‑application battery. In an isolated workspace create a dummy file, generate valid and deliberately broken unified‑diff patches (bad hunk headers, context mismatch, attempts to write outside /workspace via ../ traversal), and send them through the patch action. Confirm that valid patches are applied, that git apply failures are surfaced in the result object and captured in the JSONL log, and that no patch can escape the workspace or modify the read‑only harness volume. Also assert that every successful patch increments git history and that flip‑flop edits (same line changed and reverted) can be detected by walking the commit graph.

The observation path deserves its own suite. Populate the workspace with a directory tree deeper than two levels, add large files that should be listed but not inlined, and verify that observation_builder truncates the tree, respects hidden‑file rules, and composes the five expected keys. Artificially inflate notes.md until it exceeds eight thousand Anthropic tokens, then force an observation build and assert that the summarisation logic triggers, that the summarised block is committed with the correct message, and that a second oversize attempt yields the special {"error": "prompt_too_large"} observation.

Token‑accounting tests must compare the harness’s approximation to Anthropic’s real count. Capture an actual reply from the API, pass the identical string through the client’s count_tokens helper, and assert that the estimate is within five percent of the value returned in the API’s metadata. Run the same check for a one‑million‑character string to ensure linear scaling.

Because timing metrics are central, add wall‑clock and CPU‑time instrumentation tests. Replace the agent with a stub that sleeps for a known duration and then returns a no‑op action; verify that the think‑time field in the log matches the sleep interval and that cumulative CPU time grows negligibly. Repeat with a patch that runs an expensive local computation to confirm CPU accounting.

For network isolation, start a container whose agent attempts to open a socket to an external IP from within a patch and from within a Python subprocess; assert that the call fails with “network unreachable” and that no outbound traffic is observed on the host. Complement this with a negative test: run the same container without --network none and confirm that the connection succeeds, establishing that your flag actually enforces isolation.

The orchestrator requires concurrency and cleanup tests. Launch ten trials in parallel with batch.py, kill one container halfway through, and assert that the orchestrator records the non‑zero exit code, cleans its temporary workspace, and continues running the remaining trials. After batch completion, verify that every trial directory holds a metadata.json, container_output.txt, optional harness.log, and archived workspace; confirm that no workspace directories linger in /tmp and that docker has no residual anonymous volumes.

Persistence semantics need coverage as well. Send a sequence of actions that append scratch‑pads across three turns, overwrite notes with write_notes, and pass a message. On the following turn ensure that previous_message equals what was sent, that notes.md contains only the replaced text, and that git history shows four commits with the correct summaries. Then run an intentional crash: craft a malformed action that raises an unhandled exception in the executor, observe that the harness logs an error entry, writes a graceful termination entry with reason “turn_error”, and the container exits non‑zero.

Finally, run an end‑to‑end green‑path scenario in both control and treatment conditions. Stub the Claude client to return a predetermined series of actions that, in aggregate, bring the tests to green by editing only the target functions. Measure that the harness terminates on the first green test run, logs all required summary metrics, and, for the treatment arm, that the warning comment is indeed present in the workspace copy and absent in the control copy.

If every sentence above is expressed as an executable pytest, the resulting suite will verify build reproducibility, container hardening, action protocol strictness, patch safety, observation integrity, token budgeting, metric logging, resource accounting, persistence guarantees, orchestrator resilience, and full‑system success and failure behaviours. Only after these infrastructure tests are green should you let a real language‑model agent anywhere near the experiment.